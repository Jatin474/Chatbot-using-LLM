# Chatbot-using-LLM
Data Gathering and Preprocessing

The foundation of the chatbot lies in understanding user intents. The model input files do exactly that. The NLTK library is employed for tokenization and lemmatization of words, breaking down sentences into words and converting them into base forms.

Model Architecture

The chatbot model is built using neural networks. The model architecture comprises of three layers:

Input Layer: The input layer consists of neurons equal to the length of the bag-of-words representation of input sentences. It processes the input data.
Hidden Layers: Two dense hidden layers follow the input layer, each with ReLU activation functions. These layers are essential for learning complex patterns in the data.
Output Layer: The output layer is a dense layer with softmax activation. It predicts the output intent based on the learned patterns.

Training

The model is trained using a supervised learning approach. Training data is generated by converting input sentences into a bag-of-words representation and associating them with their corresponding intents. The model is compiled using stochastic gradient descent (SGD) with Nesterov accelerated gradient as the optimizer and categorical cross-entropy as the loss function. The training process involves iterating through epochs, and adjusting weights and biases to minimize the loss function.
